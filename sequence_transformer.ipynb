{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)5dded/.gitattributes: 100%|██████████| 1.18k/1.18k [00:00<00:00, 4.48MB/s]\n",
      "Downloading (…)_Pooling/config.json: 100%|██████████| 190/190 [00:00<00:00, 251kB/s]\n",
      "Downloading (…)4d81d5dded/README.md: 100%|██████████| 10.6k/10.6k [00:00<00:00, 24.1MB/s]\n",
      "Downloading (…)81d5dded/config.json: 100%|██████████| 573/573 [00:00<00:00, 660kB/s]\n",
      "Downloading (…)ce_transformers.json: 100%|██████████| 116/116 [00:00<00:00, 512kB/s]\n",
      "Downloading (…)ded/data_config.json: 100%|██████████| 39.3k/39.3k [00:00<00:00, 39.3MB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 134M/134M [00:17<00:00, 7.47MB/s] \n",
      "Downloading (…)nce_bert_config.json: 100%|██████████| 53.0/53.0 [00:00<00:00, 133kB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 112/112 [00:00<00:00, 139kB/s]\n",
      "Downloading (…)5dded/tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 1.47MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 352/352 [00:00<00:00, 602kB/s]\n",
      "Downloading (…)dded/train_script.py: 100%|██████████| 13.2k/13.2k [00:00<00:00, 7.78MB/s]\n",
      "Downloading (…)4d81d5dded/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 1.16MB/s]\n",
      "Downloading (…)1d5dded/modules.json: 100%|██████████| 349/349 [00:00<00:00, 1.29MB/s]\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L12-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = pd.read_csv(\"nlp-getting-started/train_correct.csv\")\n",
    "test_csv = pd.read_csv(\"nlp-getting-started/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = train_csv['text'].tolist()\n",
    "test_sentences = test_csv[\"text\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_sentences = model.encode(sentences)\n",
    "test_encoded_sentences = model.encode(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = np.array(train_csv[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_dataset = pd.DataFrame(np.concatenate([encoded_sentences, target.reshape(-1, 1)], axis=-1))\n",
    "new_test_dataset = pd.DataFrame(test_encoded_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_dataset.to_csv(\"nlp-getting-started/encoded_train.csv\", index=False)\n",
    "new_test_dataset.to_csv(\"nlp-getting-started/encoded_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
